{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hector Auvinen\\Documents\\GitHub\\adapter_experiments\\adapter_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'col_names = [\"topic\", \"retrievedUrl\", \"archivedUrl\", \"sentenceHash\", \"sentence\", \"annotation\", \"set\"]\\nfile_path = \"C:/Users/Hector Auvinen/Desktop/UKP_sentential_argument_mining/data/school_uniforms.tsv\"\\ndf = pd.read_csv(file_path,sep=\"\\t\",header=None)\\ndf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"C:/Users/Hector Auvinen/Desktop/UKP_sentential_argument_mining/data\"\n",
    "data = []\n",
    "\n",
    "# names\n",
    "\"\"\"col_names = [\"topic\", \"retrievedUrl\", \"archivedUrl\", \"sentenceHash\", \"sentence\", \"annotation\", \"set\"]\n",
    "file_path = \"C:/Users/Hector Auvinen/Desktop/UKP_sentential_argument_mining/data/school_uniforms.tsv\"\n",
    "df = pd.read_csv(file_path,sep=\"\\t\",header=None)\n",
    "df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\abortion.tsv\n",
      "3929\n",
      "\\cloning.tsv\n",
      "3039\n",
      "\\death_penalty.tsv\n",
      "3651\n",
      "\\gun_control.tsv\n",
      "3341\n",
      "\\marijuana_legalization.tsv\n",
      "2475\n",
      "\\minimum_wage.tsv\n",
      "2473\n",
      "\\nuclear_energy.tsv\n",
      "3576\n",
      "\\school_uniforms.tsv\n",
      "3008\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(file_path.split(\"data\")[-1])\n",
    "        # Read the TSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, sep='\\t',quoting=csv.QUOTE_NONE)\n",
    "        # Append data from the current file to the list\n",
    "        print(len(df))\n",
    "        data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data from all files into one DataFrame\n",
    "all_data = pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Convert the DataFrame into a Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/25492 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25492/25492 [00:00<00:00, 61148.91 examples/s]\n",
      "Filter: 100%|██████████| 25492/25492 [00:00<00:00, 70164.11 examples/s]\n",
      "Filter: 100%|██████████| 25492/25492 [00:00<00:00, 67263.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation, and test sets based on the 'set' column\n",
    "train_dataset = dataset.filter(lambda example: example['set'] == 'train')\n",
    "val_dataset = dataset.filter(lambda example: example['set'] == 'val')\n",
    "test_dataset = dataset.filter(lambda example: example['set'] == 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'set' column from the datasets\n",
    "train_dataset = train_dataset.remove_columns([\"retrievedUrl\",\"archivedUrl\",\"sentenceHash\"])\n",
    "val_dataset = val_dataset.remove_columns([\"retrievedUrl\",\"archivedUrl\",\"sentenceHash\"])\n",
    "test_dataset = test_dataset.remove_columns([\"retrievedUrl\",\"archivedUrl\",\"sentenceHash\"])\n",
    "\n",
    "# Create a dictionary containing the train, validation, and test sets\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'set' column values in train dataset\n",
    "for example in dataset_dict[\"train\"]:\n",
    "    assert example[\"set\"] == \"train\", \"Incorrect value in 'set' column for train dataset\"\n",
    "\n",
    "# Check 'set' column values in validation dataset\n",
    "for example in dataset_dict[\"validation\"]:\n",
    "    assert example[\"set\"] == \"val\", \"Incorrect value in 'set' column for validation dataset\"\n",
    "\n",
    "# Check 'set' column values in test dataset\n",
    "for example in dataset_dict[\"test\"]:\n",
    "    assert example[\"set\"] == \"test\", \"Incorrect value in 'set' column for test dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "{'topic': 'abortion', 'sentence': 'Where did you get that ?', 'annotation': 'NoArgument', 'set': 'train'}\n",
      "\n",
      "Validation dataset:\n",
      "{'topic': 'abortion', 'sentence': 'This means it has to steer monetary policy to ( a ) keep prices stable , and to ( b ) keep unemployment low and the economy growing .', 'annotation': 'NoArgument', 'set': 'val'}\n",
      "\n",
      "Test dataset:\n",
      "{'topic': 'abortion', 'sentence': 'With that I would like to give everyone something to contemplate .', 'annotation': 'NoArgument', 'set': 'test'}\n"
     ]
    }
   ],
   "source": [
    "# Display the first few examples from each split\n",
    "print(\"Train dataset:\")\n",
    "print(dataset_dict[\"train\"][0])\n",
    "print(\"\\nValidation dataset:\")\n",
    "print(dataset_dict[\"validation\"][0])\n",
    "print(\"\\nTest dataset:\")\n",
    "print(dataset_dict[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 18341/18341 [00:00<00:00, 103314.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2042/2042 [00:00<00:00, 73123.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5109/5109 [00:00<00:00, 75333.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "write_path = \"C:/Users/Hector Auvinen/Desktop/UKP_sentential_argument_mining/hf_data/argument_mining\"\n",
    "\n",
    "dataset_dict.save_to_disk(write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_from_disk\n",
    "loaded_dataset = load_from_disk(write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'sentence', 'annotation', 'set'],\n",
       "        num_rows: 18341\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['topic', 'sentence', 'annotation', 'set'],\n",
       "        num_rows: 2042\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['topic', 'sentence', 'annotation', 'set'],\n",
       "        num_rows: 5109\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
